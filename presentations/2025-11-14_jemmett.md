## # Practical Testing for Reproducible Analytical Pipelines (Tom Jemmett)

### Links

- [Slides](https://the-strategy-unit.github.io/data_science/presentations/2025-11-14_unit-testing/)

- [Code](https://github.com/The-Strategy-Unit/data_science/tree/main/presentations/2025-11-14_unit-testing)

### Abstract

As data analysts and scientists, we rely on code to clean, transform, and model data. You may have read about the importance of testing—for example, as a key step towards achieving a Silver Reproducible Analytical Pipeline (RAP). But you’ve probably also discovered how difficult it can be to write useful, meaningful tests as your codebase becomes more complex.

Even if you manage to write some tests, you might hit another wall: running them in GitHub Actions or other CI systems when they depend on internal databases or live APIs. Without care, your “reproducible” pipeline can end up brittle and hard to maintain.

This talk will show practical strategies you can use to make your code easier to test. We’ll look at simple patterns for structuring and abstracting analysis code so that tests become straightforward rather than painful. I’ll also introduce mocking—a powerful technique for simulating external dependencies like databases, APIs, or large datasets. With mocking, you can run fast, reliable tests anywhere, even without access to production systems.

You’ll leave with concrete tips to improve your own pipelines: cleaner code, simpler tests, and a smoother path to truly reproducible analytical workflows.